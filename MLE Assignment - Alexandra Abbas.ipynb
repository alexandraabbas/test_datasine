{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Given a sentence, detect the level of formality and informativeness. <br><br>\n",
    "In data science terms, it is a regression problem rather than a classification problem, since the goal is to predict continuous variables (formality, informativeness) from text data. <br><br>\n",
    "I used Linear Regression to solve the problem. Linear Regression is quick and easy to implement, as well as well suited to work with sparse data like text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data summary\n",
    "SQUINKY Dataset contains sentences and corresponding annotations, one sentence per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence ID</th>\n",
       "      <th>Formality</th>\n",
       "      <th>Informativeness</th>\n",
       "      <th>Implicature</th>\n",
       "      <th>Length in Words</th>\n",
       "      <th>Length in Characters</th>\n",
       "      <th>F-score</th>\n",
       "      <th>I-score</th>\n",
       "      <th>Lexical Density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7032.000000</td>\n",
       "      <td>7032.000000</td>\n",
       "      <td>7032.000000</td>\n",
       "      <td>7032.000000</td>\n",
       "      <td>7032.000000</td>\n",
       "      <td>7032.000000</td>\n",
       "      <td>7032.000000</td>\n",
       "      <td>7032.000000</td>\n",
       "      <td>7032.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3510.124005</td>\n",
       "      <td>3.758817</td>\n",
       "      <td>4.580063</td>\n",
       "      <td>3.941382</td>\n",
       "      <td>19.213879</td>\n",
       "      <td>111.665956</td>\n",
       "      <td>63.422057</td>\n",
       "      <td>4.975162</td>\n",
       "      <td>64.204264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2031.328248</td>\n",
       "      <td>1.311312</td>\n",
       "      <td>1.184735</td>\n",
       "      <td>0.926688</td>\n",
       "      <td>11.737192</td>\n",
       "      <td>70.119067</td>\n",
       "      <td>18.213751</td>\n",
       "      <td>2.069756</td>\n",
       "      <td>12.656599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1751.750000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>53.846154</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>57.072829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3509.500000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>65.909091</td>\n",
       "      <td>4.812500</td>\n",
       "      <td>63.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5269.250000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>75.862069</td>\n",
       "      <td>6.071429</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7027.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>810.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence ID    Formality  Informativeness  Implicature  \\\n",
       "count  7032.000000  7032.000000      7032.000000  7032.000000   \n",
       "mean   3510.124005     3.758817         4.580063     3.941382   \n",
       "std    2031.328248     1.311312         1.184735     0.926688   \n",
       "min       0.000000     1.000000         1.000000     1.000000   \n",
       "25%    1751.750000     2.600000         3.800000     3.400000   \n",
       "50%    3509.500000     3.800000         4.800000     4.000000   \n",
       "75%    5269.250000     4.800000         5.400000     4.600000   \n",
       "max    7027.000000     7.000000         7.000000     6.800000   \n",
       "\n",
       "       Length in Words  Length in Characters      F-score      I-score  \\\n",
       "count      7032.000000           7032.000000  7032.000000  7032.000000   \n",
       "mean         19.213879            111.665956    63.422057     4.975162   \n",
       "std          11.737192             70.119067    18.213751     2.069756   \n",
       "min           1.000000              3.000000   -25.000000     0.000000   \n",
       "25%          10.000000             59.000000    53.846154     3.700000   \n",
       "50%          18.000000            101.000000    65.909091     4.812500   \n",
       "75%          26.000000            152.000000    75.862069     6.071429   \n",
       "max         150.000000            810.000000   150.000000    22.000000   \n",
       "\n",
       "       Lexical Density  \n",
       "count      7032.000000  \n",
       "mean         64.204264  \n",
       "std          12.656599  \n",
       "min           0.000000  \n",
       "25%          57.072829  \n",
       "50%          63.333333  \n",
       "75%          70.000000  \n",
       "max         200.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pandas.read_csv('fii_annotations/mturk_experiment_1.csv', encoding = \"ISO-8859-1\")\n",
    "pandas.DataFrame.describe(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "I created the data splits for training (80%) and testing (20%). <br><br>\n",
    "I formatted the sentences in the training set such that all characters are lowercase and they consist of letters and digits only. Then, I constructed the TF-IDF feature vectors. TF-IDF vectors are modified versions of bag of words vectors where the concept of inverse document frequecy (IDF) is used, rather than using frequencies of distinct words in a sentence. IDF diminishes the weights of the most frequently occuring words, e.g. \"the\", in a document and gives weight to less frequently occuring terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test splits\n",
    "train, test = train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform train['Sentence'] to lowercase\n",
    "train['Sentence'].str.lower()\n",
    "\n",
    "# replace punctuation and special characters\n",
    "train['Sentence'].replace('[^a-zA-Z0-9]', ' ', regex = True)\n",
    "\n",
    "# convert sentences to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(min_df = 5)\n",
    "X_tfidf = vectorizer.fit_transform(train['Sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "Ridge regression is a modified version of linear regression where rather than Ordinary Least Squares, the cost function is expanded with a penalty term, called L2 regularisation. It shrinks the parameters therefore handles highly correlated variables and produces a more reliable model. I built two regressors for independently predicting Formality and Informativeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=241, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build models for predicting Formality (rgs_f) and Informativeness (rgs_i)\n",
    "rgs_f = Ridge(alpha=1.0, random_state=241) # alpha = regularization strength\n",
    "rgs_i = Ridge(alpha=1.0, random_state=241) # random_state = seed to use when shuffling the data\n",
    "\n",
    "# target values\n",
    "y_f = train['Formality']\n",
    "y_i = train['Informativeness']\n",
    "\n",
    "# train models on training data\n",
    "rgs_f.fit(X_tfidf, y_f)\n",
    "rgs_i.fit(X_tfidf, y_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Similarly to the training set, data preparation was done on the test set as well. Both models were evaulated on the test data. Evaluation metrics used are mean absolute error (MAE) and root mean squared error (RMSE). MAE measures the average value of error across all predictions without considering the direction of error. RMSE also measures the average size of error by taking the square root of the average of squared differences between the prediction and the actual values. It gives more weight to large errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test['Sentence'] to lowercase\n",
    "test['Sentence'].str.lower()\n",
    "\n",
    "# replace punctuation and special characters\n",
    "test['Sentence'].replace('[^a-zA-Z0-9]', ' ', regex = True)\n",
    "\n",
    "# convert sentences to a matrix of TF-IDF features\n",
    "X_test = vectorizer.transform(test['Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formality - results:\n",
      "MAE: 0.7463534878973395\n",
      "RMSE: 0.9319665473156428\n",
      "Informativeness - results:\n",
      "MAE: 0.692765152360049\n",
      "RMSE: 0.8700044869568727\n"
     ]
    }
   ],
   "source": [
    "# evaluate models on test data\n",
    "rslt_f = rgs_f.predict(X_test)\n",
    "rslt_i = rgs_i.predict(X_test)\n",
    "\n",
    "# print evaluation metrics\n",
    "print('Formality - results:')\n",
    "print('MAE:', mean_absolute_error(test['Formality'], rslt_f)) # mean absolute error regression loss\n",
    "print('RMSE:', sqrt(mean_squared_error(test['Formality'], rslt_f))) # root mean squared error regression loss\n",
    "\n",
    "print('Informativeness - results:')\n",
    "print('MAE:', mean_absolute_error(test['Informativeness'], rslt_i)) \n",
    "print('RMSE:', sqrt(mean_squared_error(test['Informativeness'], rslt_i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to MAE, the average deviation in predicting Formality is 0.75 and in predicting Infomativeness is 0.69. Given that the scale of the variables is 1.0 to 7.0, the model gives a good estimate of the level of Formality and Infomativeness in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> Naturally, in a real life scenario, several models and different hyperparameter settings would have been evaluated using validation, however in this instance, due to time constraints, only a single model and a single hyperparameter setting were used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Besides just predictive accuracy, what useful insights could you use this model to produce which would help a user alter the formality or informativeness of a piece of text that theyâ€™ve provided?\n",
    "I think there are two levels to provide value for a user.<br><br>\n",
    "\n",
    "*Indication*: Highlight sentences (or even words) using color codes to indicate how those are impacting the formality or informativeness scores.<br><br>\n",
    "\n",
    "*Indication + Suggestions*: Same as the previous, but also suggest alternative options for the user to alter the formality or informativeness of a piece of text.<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
